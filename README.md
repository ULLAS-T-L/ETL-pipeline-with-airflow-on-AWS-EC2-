****How to build and automate a python ETL pipeline with airflow on AWS EC2 | Data Engineering Project****

In the course of this data engineering project, I acquired the skills to construct and automate an ETL (Extract, Transform, Load) process with the following steps:

1. **Step 1:**
   - Extracting current weather data from the OpenWeatherMap API.

2. **Step 2:**
   - Transforming the extracted data and subsequently loading it into an S3 bucket, all orchestrated through Apache Airflow.

3. **Step 3:**
   - Gaining proficiency in Apache Airflow concepts such as Directed Acyclic Graphs (DAGs) and Operators.


Throughout this project, I acquired knowledge in the following areas:

1. **AWS:**
   - Explored AWS services such as IAM rules, EC2 instances, Airflow installation, S3 usage, and establishing SSH connections from Visual Studio Code to AWS.

2. **Airflow:**
   - Gained insights into Airflow, encompassing a deeper understanding of operators and the creation of Directed Acyclic Graphs (DAGs).

3. **ETL (Extract, Transform, Load):**
   - Executed ETL processes involving extraction from an API, transformation using Python scripts, and loading the transformed data into an S3 bucket.


![Architecture](https://github.com/ULLAS-T-L/ETL-pipeline-with-airflow-on-AWS-EC2-/blob/04889b9e8a0134ded04f8f218226e8babbc8b9ca/Architecture.png)




Credit - tuplespectra 
